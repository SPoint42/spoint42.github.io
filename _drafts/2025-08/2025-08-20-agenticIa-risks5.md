---
layout: post
date: 2025-08-20 06:00:00 +0200
title: "üßë‚Äçüç≥ Agentic AI : 5 Risques Principaux des Agents IA -- Action Malveillante Autonome ü§ñ"
categories:
  - Fiche-Pratique
  - CyberSec
  - OWASP
  - LLM
  - GenAI
  - AgenticAI
---



Ce risque est le plus complexe. Il ne d√©coule ni d'une attaque directe, ni d'un exc√®s de z√®le, mais d'une **erreur de
raisonnement fondamentale** de l'agent. Face √† un objectif ambigu ou √† des donn√©es qu'il interpr√®te mal, l'agent √©labore
et ex√©cute de mani√®re autonome un plan d'action qui s'av√®re √™tre destructeur.

#### **Exemple de Code Python**

Un agent est charg√© d'optimiser une application en "nettoyant les logs inutiles".

```python
import os

FILESYSTEM = {
    "/logs/app.log": "contenu de 20MB",
    "/logs/archive-2024.zip": "contenu de 500MB",
    "/logs/db_backup.sql.gz": "contenu de 2GB"
}


def list_files(path):
    return [f for f in FILESYSTEM.keys() if f.startswith(path)]


def get_file_size(file_path):
    if "MB" in FILESYSTEM[file_path]:
        return int(FILESYSTEM[file_path].split("MB")[0])
    if "GB" in FILESYSTEM[file_path]:
        return int(FILESYSTEM[file_path].split("GB")[0]) * 1024
    return 0


def delete_file(file_path):
    print(f"üî• ACTION DESTRUCTIVE : Suppression de {file_path}")
    del FILESYSTEM[file_path]
    return f"{file_path} supprim√©."


def agent_optimizer(goal):
    print(f"\n[Agent] Objectif re√ßu : '{goal}'")

    # 1. Phase de raisonnement : l'agent √©labore un plan
    print(
        "[Agent] Plan : 1. Lister les fichiers dans /logs. 2. Identifier les plus gros. 3. Les supprimer pour lib√©rer de l'espace.")

    # 2. Phase d'ex√©cution
    log_files = list_files("/logs/")
    files_with_sizes = [(f, get_file_size(f)) for f in log_files]
    files_with_sizes.sort(key=lambda x: x[1], reverse=True)

    # Le raisonnement est erron√© : l'agent assimile "gros fichier dans /logs" √† "log inutile"
    file_to_delete = files_with_sizes[0][0]
    print(
        f"[Agent] Inf√©rence erron√©e : Le fichier le plus volumineux est '{file_to_delete}'. C'est donc un log inutile.")

    # 3. Action
    delete_file(file_to_delete)
    print(f"[Agent] R√©sultat : {FILESYSTEM}")


# Objectif ambigu donn√© par un op√©rateur
ambiguous_goal = "Lib√®re de l'espace disque en nettoyant les logs."
agent_optimizer(ambiguous_goal)

```

Ici, la mission "nettoyer les logs" √©tait trop vague. L'agent a correctement identifi√© le fichier le plus volumineux,
mais son raisonnement a √©chou√© en concluant √† tort qu'il s'agissait d'un log √† supprimer, alors qu'il s'agissait d'une
sauvegarde critique de la base de donn√©es.

#### **Exemple R√©el**

Bien qu'il n'y ait pas encore de CVE publique c√©l√®bre pour ce cas pr√©cis (car il rel√®ve plus d'un √©chec de conception
que d'une vuln√©rabilit√© exploitable de l'ext√©rieur), c'est un sujet de pr√©occupation majeur dans la recherche en
s√©curit√© IA. Les "hallucinations" des LLM en sont une forme primitive. Un agent agissant sur une hallucination (par
exemple, en croyant qu'une API inexistante existe et en tentant de l'appeler en boucle) pourrait causer un d√©ni de
service. Le risque d'action autonome malveillante est la raison pour laquelle la plupart des syst√®mes d'agents actuels
n√©cessitent une **validation humaine** avant toute action critique.
