---
layout: post
title: "L'Injection Indirecte ou Contextuelle de Prompt : Comprendre les Menaces üîó"
date: 2025-02-27
categories: [S√©curit√©, IA]
---

L'injection indirecte ou contextuelle de prompt est une technique plus subtile o√π les attaquants exploitent le contexte de la conversation pour manipuler le comportement d'un Large Language Model (LLM). Contrairement √† l'injection directe, cette m√©thode ne repose pas sur des instructions explicites mais sur la manipulation du contexte pour inciter le mod√®le √† r√©v√©ler des informations sensibles ou √† adopter un comportement non souhait√©.

## Comment Fonctionne l'Injection Indirecte de Prompt ?

- **Description** : L'utilisateur structure ses questions ou ses demandes de mani√®re √† induire le mod√®le en erreur, en le poussant √† r√©v√©ler des informations ou √† adopter un comportement non souhait√© sans utiliser d'instructions explicites.
- **Exemple** : Un utilisateur pourrait poser une s√©rie de questions apparemment inoffensives qui, mises bout √† bout, r√©v√®lent des informations sensibles ou permettent de d√©duire des donn√©es confidentielles.

## Exemple de Faille Connue

- **CVE-2024-5565** : Ex√©cution de Code via Injection de Prompt dans Vanna.AI. Cette vuln√©rabilit√© permet l'ex√©cution de code √† distance via des techniques d'injection de prompt dans la biblioth√®que Vanna.AI, qui offre une interface texte-√†-SQL pour les utilisateurs. Les attaquants peuvent contourner les garanties de "pre-prompting" pour ex√©cuter des commandes arbitraires.
    - **Impact** : Cette faille peut √™tre exploit√©e pour ex√©cuter des commandes non autoris√©es sur le syst√®me h√¥te, posant un risque majeur pour la s√©curit√© des applications utilisant Vanna.AI.
    - **R√©f√©rence** : [JFrog Blog - CVE-2024-5565](https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/)

## Comment Se Prot√©ger ?

### 1. **Validation et Sanitization des Entr√©es**

Il est crucial de valider et de nettoyer toutes les entr√©es utilisateur pour d√©tecter et rejeter les tentatives d'injection de prompt. Cela peut inclure la limitation de la longueur des entr√©es et l'utilisation de listes blanches pour les formats accept√©s.

### 2. **Contr√¥les d'Acc√®s Robustes**

Mettre en place des contr√¥les d'acc√®s stricts pour limiter les actions que le LLM peut entreprendre. Utiliser des r√¥les et des permissions pour restreindre l'acc√®s aux fonctionnalit√©s sensibles.

### 3. **Surveillance et Journalisation**

Impl√©menter des m√©canismes de surveillance pour d√©tecter les comportements anormaux. La journalisation des interactions peut aider √† identifier et √† r√©pondre rapidement aux tentatives d'injection.

En comprenant ces risques et en mettant en ≈ìuvre des mesures de s√©curit√© appropri√©es, les d√©veloppeurs et les organisations peuvent prot√©ger leurs syst√®mes contre ces attaques.
