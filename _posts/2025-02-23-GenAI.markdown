---
layout: post
title:  "üöÄ S√©curit√© de la GEN AI && OWASP !"
date:   2025-02-23 00:00:00 +0100
categories: veille security owasp genAI 
lang : french
---

Ce post est le premier d'une s√©rie de plusieurs articles autour des risques et de comment l'OWASP peut aider a prot√©ger des syst√®mes de Generative AI . Il fait suite a la pr√©sentation que j'ai pu effectuer en D√©cembre 2024 √† [Hack-it-n](https://hack-it-n.com/), et va continuer sur diff√©rents sujets autour de cette GEN AI

Nous commencons par pr√©sentez rapidement le Guide  [OWASP Top 10: LLM & Generative AI Security Risks](https://genaisecurityproject.com/llm-top-10/). 


Avec l'essor rapide des technologies d'intelligence artificielle g√©n√©rative (GEN AI), les d√©fis en mati√®re de s√©curit√© deviennent de plus en plus complexes. 
L'OWASP a donc  publi√© une mise a jour du guide complet pour aider les organisations √† naviguer dans ce paysage en constante √©volution.

üîí Pourquoi ce guide est-il crucial dans votre d√©ploiement d'une solution de GEN AI?

üîí 1. Apport d'√©l√©ments Cl√©s et d'un r√©f√©renttiel  pour le Testing en Red Teaming d'une  GEN AI :
Le guide OWASP propose une approche structur√©e pour √©valuer les vuln√©rabilit√©s des syst√®mes GEN AI √† travers le Red Teaming. Cette m√©thode permet de 
simuler des attaques r√©elles pour tester la robustesse des d√©fenses mises en place.

- Pourquoi est-ce important ? Le Red Teaming aide √† identifier les failles de s√©curit√© avant qu'elles ne soient exploit√©es par des attaquants, permettant 
ainsi aux organisations de renforcer leurs mesures de protection.
- Exemple concret : Une √©quipe de Red Teaming pourrait tester un mod√®le de traitement du langage naturel pour voir s'il peut √™tre manipul√© pour divulguer 
des informations sensibles.



üîÑ 2. S√©curisation des Applications GEN AI Tout au Long du Cycle de Vie LLMSecOps :
L'OWASP met en avant l'importance de s√©curiser les applications GEN AI √† chaque √©tape de leur cycle de vie, de la conception √† la mise en ≈ìuvre et √† la 
maintenance.

- Pourquoi est-ce important ? Une approche int√©gr√©e de la s√©curit√© permet de minimiser les risques et de garantir que les applications GEN AI sont 
robustes et fiables.
- Exemple concret : Impl√©menter des contr√¥les de s√©curit√© d√®s la phase de conception, comme l'anonymisation des donn√©es sensibles, peut pr√©venir les 
fuites d'informations lors du d√©ploiement.

üë• 3. Con√ßu pour les  CISO, D√©veloppeurs, Data Scientists et Experts en S√©curit√© :

Le guide est destin√© √† un large √©ventail de professionnels impliqu√©s dans le d√©veloppement et la gestion des technologies GEN AI. Il offre des 
recommandations pratiques adapt√©es √† chaque r√¥le.

- Pourquoi est-ce important ? Une approche collaborative permet de s'assurer que toutes les parties prenantes sont align√©es sur les meilleures pratiques 
en mati√®re de s√©curit√©.
- Exemple concret : Les d√©veloppeurs peuvent utiliser le guide pour impl√©menter des techniques de codage s√©curis√©, tandis que les experts en s√©curit√© 
peuvent s'en servir pour auditer les syst√®mes GEN AI. 


üö® Exemples de Sc√©narios de risques concrets et leur cons√©quence :

1. Fuites de Donn√©es Sensibles :
	- Sc√©nario : Une application de sant√© utilisant GEN AI pour analyser des dossiers m√©dicaux pourrait accidentellement exposer des informations 
	personnelles sensibles si les donn√©es ne sont pas correctement anonymis√©es ou prot√©g√©es.
	- Cons√©quence : Violation de la confidentialit√© des patients et non-conformit√© aux r√©glementations comme le RGPD.
	- Exemple concret : Une vuln√©rabilit√© de travers√©e de r√©pertoire ([CWE-22](https://www.cvedetails.com/cwe-details/22/Improper-Limitation-of-a-Pathname-to-a-Restricted-Directory-.html) ) dans la fonction de t√©l√©chargement d'utilisateur de 
	ChuanhuChatGPT, identifi√©e comme [CVE-2024-5982](https://www.cvedetails.com/cve/CVE-2024-5982/), a permis l'ex√©cution arbitraire de code, la 
	cr√©ation de r√©pertoires et l'exposition de donn√©es sensibles 

2. Manipulation par Deepfakes :
	- Sc√©nario : Des deepfakes r√©alistes peuvent √™tre utilis√©s pour diffuser de fausses informations ou manipuler des individus. Par exemple, 
	une vid√©o deepfake d'un PDG annon√ßant une fausse acquisition pourrait influencer le march√© boursier.
	- Cons√©quence : Perte de confiance du public et impact financier potentiellement d√©sastreux.
	- Exemple concret : La chanteuse [Taylor Swift](https://intelligence-artificielle.com/taylor-swift-victime-deepfake/) a √©t√© victime de deepfakes pornographiques diffus√©s sur la plateforme X, g√©n√©r√©s via 
	Microsoft Designer par un groupe Telegram. 

3. Attaques par Injection de Prompts :
	- Sc√©nario : Un attaquant pourrait injecter des instructions malveillantes dans les prompts utilis√©s par un mod√®le GEN AI, alt√©rant ainsi son 
	comportement.
	- Cons√©quence : Compromission de la s√©curit√© du syst√®me et ex√©cution non autoris√©e d'actions.
	- Exemple concret : La [CVE-2024-5565](https://github.com/advisories/GHSA-7735-w2jp-gvg6) est une vuln√©rabilit√©  de prompt injection dans la 
	biblioth√®que Vanna.AI, permettant l'ex√©cution de commandes arbitraires √† distance. Cette faille expose les bases de donn√©es √† des risques d'ex√©cution 
	de code malveillant.


üåê En savoir plus sur le guide : [OWASP Top 10: LLM & Generative AI Security Risks](https://genaisecurityproject.com/llm-top-10/)
